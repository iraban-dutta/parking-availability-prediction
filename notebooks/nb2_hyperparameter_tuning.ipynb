{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98813e6-7415-450a-8690-17f5a831d1fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "100f91cd-6f89-42e2-83cf-5d95597f4440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "969502ad-a379-47c9-8b7a-6ae0d32b9c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import pmdarima as pm\n",
    "from sklearn.metrics import mean_squared_error as mse, mean_absolute_error as mae, mean_absolute_percentage_error as mape\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "\n",
    "from statsmodels.tools.sm_exceptions import ValueWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ValueWarning)\n",
    "\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66fae025-df8d-409c-ab42-159f596c7210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422a3fc3-717d-4fa1-8f67-cc11cc01ab27",
   "metadata": {},
   "source": [
    "# Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99b4ec4f-32b0-410e-baef-3cece3a510f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37422, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ts_final = pd.read_csv('./data_artifacts/df_ts_final.csv')\n",
    "df_ts_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e16dbb4-e525-47a3-b482-b59c34f83e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data_artifacts/ts_train_test_dict.pkl', 'rb') as f:\n",
    "    ts_train_test_splitted_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e99aef80-a005-4af1-8934-60a042853edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data_artifacts/reg_v1_train_test_dict.pkl', 'rb') as f:\n",
    "    reg_final_v1_train_test_dict = pickle.load(f)\n",
    "    \n",
    "with open('./data_artifacts/reg_v2_train_test_dict.pkl', 'rb') as f:\n",
    "    reg_final_v2_train_test_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "724cd52d-c1cf-4674-9e9d-6547606e1f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1260, 14)\n",
      "(126, 14)\n",
      "(1260, 15)\n",
      "(126, 15)\n"
     ]
    }
   ],
   "source": [
    "# Dropping 'Capacity' columns\n",
    "def drop_cols(reg_train_test_dict, col):\n",
    "\n",
    "    reg_mod_train_test_dict = {}\n",
    "    for ps_idx in list(reg_train_test_dict.keys())[:]:\n",
    "\n",
    "        df_train = reg_train_test_dict[ps_idx]['train']\n",
    "        df_test = reg_train_test_dict[ps_idx]['test']\n",
    "\n",
    "        reg_mod_train_test_dict[ps_idx] = {}\n",
    "        reg_mod_train_test_dict[ps_idx]['train'] = df_train.drop([col], axis=1)\n",
    "        reg_mod_train_test_dict[ps_idx]['test'] = df_test.drop([col], axis=1)\n",
    "        \n",
    "    return reg_mod_train_test_dict\n",
    "    \n",
    "\n",
    "reg_final_v1_mod_train_test_dict = drop_cols(reg_train_test_dict=reg_final_v1_train_test_dict, col='Capacity')\n",
    "reg_final_v2_mod_train_test_dict = drop_cols(reg_train_test_dict=reg_final_v2_train_test_dict, col='Capacity') \n",
    "    \n",
    "print(reg_final_v1_mod_train_test_dict[1]['train'].shape)\n",
    "print(reg_final_v1_mod_train_test_dict[1]['test'].shape)\n",
    "print(reg_final_v2_mod_train_test_dict[1]['train'].shape)\n",
    "print(reg_final_v2_mod_train_test_dict[1]['test'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5838515b-2a6b-4834-bfcd-d68f22153e70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8f72d4-2d6f-4da3-902c-ef89fb9e12a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ed4db1e-112d-41ec-a935-caaf54d2c1fa",
   "metadata": {},
   "source": [
    "# Time based Train-Test Split across All Parking Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c718408-982f-4ae1-98c3-04fac4c8b73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts_train_test_splitted_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16cae859-71e1-4cc9-b390-0a7e58e0fd04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStamp\n",
       "2016-10-04 08:00:00    10.5719\n",
       "2016-10-04 08:30:00    11.0919\n",
       "2016-10-04 09:00:00    13.8648\n",
       "2016-10-04 09:30:00    18.5442\n",
       "2016-10-04 10:00:00    25.9965\n",
       "                        ...   \n",
       "2016-12-12 14:30:00    28.5962\n",
       "2016-12-12 15:00:00    27.5563\n",
       "2016-12-12 15:30:00    22.7036\n",
       "2016-12-12 16:00:00    19.9307\n",
       "2016-12-12 16:30:00    16.4645\n",
       "Name: Occupancy_Rate, Length: 1260, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1st Parking Lot: Train Set\n",
    "ts_train_test_splitted_dict[1]['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01c871ef-cb76-46ad-8a5e-8181a7ccf5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStamp\n",
       "2016-12-13 08:00:00     2.4263\n",
       "2016-12-13 08:30:00     3.4662\n",
       "2016-12-13 09:00:00     5.7192\n",
       "2016-12-13 09:30:00    10.2253\n",
       "2016-12-13 10:00:00    15.7712\n",
       "                        ...   \n",
       "2016-12-19 14:30:00    53.5529\n",
       "2016-12-19 15:00:00    51.9931\n",
       "2016-12-19 15:30:00    47.4870\n",
       "2016-12-19 16:00:00    39.8614\n",
       "2016-12-19 16:30:00    33.4489\n",
       "Name: Occupancy_Rate, Length: 126, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1st Parking Lot: Test Set\n",
    "ts_train_test_splitted_dict[1]['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab2cbf9-30b1-4a4f-99bb-7b3e180600cc",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b959dfdf-bf8d-4da7-bcff-823ef58b9280",
   "metadata": {},
   "source": [
    "## Evaluation Metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df7951d9-e5b7-445f-87dc-6fdafa5ce8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to print values of all these metrics.\n",
    "def ts_performance_metrics(actual, predicted):\n",
    "    print('-'*50)\n",
    "    print('Forecasting Metrics')\n",
    "    print('-'*50)\n",
    "    metric_mae = round(mae(actual, predicted), 3)\n",
    "    metric_rmse = round(mse(actual, predicted)**0.5, 3)\n",
    "    print('MAE :', metric_mae)\n",
    "    print('RMSE :', metric_rmse) \n",
    "    # print('MAPE:', round(mape(actual, predicted), 3))\n",
    "    \n",
    "    return metric_mae, metric_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6bbc54-fa6e-4471-bb6d-25fe6ce6cb2d",
   "metadata": {},
   "source": [
    "# HyperParameter Tuning: Exponential Smoothing (TES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d606de3f-b80f-4719-a8e3-9afef23732c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_exp_smooth(train_data, cv_folds, alpha_vals, beta_vals, gamma_vals, season_period):\n",
    "    \n",
    "    dict_hyp = {'alpha':[], 'beta':[], 'gamma':[], 'cv_rmse':[], 'cv_mae':[]}\n",
    "    \n",
    "    # Use the full training data for cross-validation within the training set\n",
    "    tscv = TimeSeriesSplit(n_splits=cv_folds)\n",
    "    \n",
    "    # Trying out different hyper-parameters\n",
    "    for alpha in alpha_vals:\n",
    "        for beta in beta_vals:\n",
    "            for gamma in gamma_vals:\n",
    "        \n",
    "                cv_rmse_lst = []\n",
    "                cv_mae_lst = []\n",
    "\n",
    "                # Performing cross-validation with the training set\n",
    "                for train_index, val_index in tscv.split(train_data):\n",
    "                    train_fold = train_data.iloc[train_index]\n",
    "                    val_fold = train_data.iloc[val_index]\n",
    "\n",
    "                    # Fit Exponential Smoothing model with the current alpha\n",
    "                    model = ExponentialSmoothing(train_fold, trend='add', seasonal='add', \n",
    "                                                 seasonal_periods=season_period).fit(smoothing_level=alpha,                       \n",
    "                                                                                     smoothing_trend=beta, \n",
    "                                                                                     smoothing_seasonal=gamma)\n",
    "                    \n",
    "\n",
    "                    # Forecast for the validation set\n",
    "                    forecast = model.forecast(steps=len(val_fold))\n",
    "                    \n",
    "                    # # Debug\n",
    "                    # print(alpha, beta, gamma, len(val_fold), len(forecast), sum(np.isnan(val_fold)), sum(np.isnan(forecast)))\n",
    "\n",
    "                    if sum(np.isnan(forecast)) == len(val_fold): \n",
    "                        # Ensuring forecasts are not all nans\n",
    "                        continue\n",
    "                    else:\n",
    "                        # Compute validation error (RMSE)\n",
    "                        rmse_fold = round(mse(val_fold, forecast)**0.5, 3)\n",
    "                        mae_fold = round(mae(val_fold, forecast), 3)\n",
    "\n",
    "                        # Append RMSE & MAE to cv_errors\n",
    "                        cv_rmse_lst.append(rmse_fold)\n",
    "                        cv_mae_lst.append(mae_fold)\n",
    "\n",
    "\n",
    "                # Average validation error for current hyperparameter combination\n",
    "                dict_hyp['alpha'].append(alpha)\n",
    "                dict_hyp['beta'].append(beta)\n",
    "                dict_hyp['gamma'].append(gamma)\n",
    "                dict_hyp['cv_rmse'].append(round(np.mean(cv_rmse_lst), 3))\n",
    "                dict_hyp['cv_mae'].append(round(np.mean(cv_mae_lst), 3))\n",
    "      \n",
    "    # Finding the combination with lowest rmse\n",
    "    df_hyp = pd.DataFrame(dict_hyp).sort_values(by='cv_rmse')\n",
    "    df_hyp_best_combo_dict = df_hyp.iloc[0].to_dict()\n",
    "    print(f'Best parameters found: {df_hyp_best_combo_dict}')\n",
    "    \n",
    "    return df_hyp_best_combo_dict\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7567adb8-8d89-45b1-9469-30082d16fc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_exp_smooth_all_park_lots(train_test_splitted_dict, cv, alpha_values, beta_values, gamma_values,  seasonal_period):\n",
    "    \n",
    "    tuned_exp_smooth_models_dict = {'ps_idx':[], 'alpha':[], 'beta':[], 'gamma':[], \n",
    "                                    'cv_rmse': [], 'cv_mae': []}\n",
    "\n",
    "    for ps_idx in list(train_test_splitted_dict.keys())[:]:\n",
    "\n",
    "        print(f'Finding Best Parameters for Park_Space_ID: {ps_idx}')\n",
    "        print('-'*50)\n",
    "        train_series = ts_train_test_splitted_dict[ps_idx]['train']\n",
    "        test_series = ts_train_test_splitted_dict[ps_idx]['test']\n",
    "        print(f'Train series length: {train_series.shape[0]},  Test series length: {test_series.shape[0]}')\n",
    "\n",
    "        # Start Tuning\n",
    "        # Define s\n",
    "        alpha_values = np.round(np.arange(0, 0.9, 0.15), 2)  # Alpha values from 0 to 0.9\n",
    "        beta_values = np.round(np.arange(0, 0.9, 0.15), 2)  # Beta values from 0 to 0.9\n",
    "        gamma_values = np.round(np.arange(0, 0.9, 0.15), 2)  # Gamma values from 0 to 0.9\n",
    "        \n",
    "        \n",
    "        print('Finding best parameters start')\n",
    "        start_time = time.time()\n",
    "\n",
    "        tes_hyp_dict = cross_val_exp_smooth(train_data=train_series, \n",
    "                                    cv_folds=cv, \n",
    "                                    alpha_vals=alpha_values, \n",
    "                                    beta_vals=beta_values,     \n",
    "                                    gamma_vals=gamma_values, \n",
    "                                    season_period=seasonal_period)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        time_taken = (end_time-start_time)\n",
    "        print('Finding best parameters end')\n",
    "        # End Tuning\n",
    "\n",
    "        \n",
    "        # Storing the results\n",
    "        tuned_exp_smooth_models_dict['ps_idx'].append(ps_idx)\n",
    "        tuned_exp_smooth_models_dict['alpha'].append(tes_hyp_dict['alpha'])\n",
    "        tuned_exp_smooth_models_dict['beta'].append(tes_hyp_dict['beta'])\n",
    "        tuned_exp_smooth_models_dict['gamma'].append(tes_hyp_dict['gamma'])\n",
    "        tuned_exp_smooth_models_dict['cv_rmse'].append(tes_hyp_dict['cv_rmse'])\n",
    "        tuned_exp_smooth_models_dict['cv_mae'].append(tes_hyp_dict['cv_mae'])\n",
    "\n",
    "        print(('-'*50)+f'Completed, Time Taken: {round(time_taken, 2)} '+('-'*50))\n",
    "        \n",
    "        \n",
    "    return pd.DataFrame(tuned_exp_smooth_models_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdde4e85-1d41-4530-ba75-74fdea4fd0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define parameter search space\n",
    "# alpha_vals = np.round(np.arange(0, 0.9, 0.15), 2)  # Alpha values from 0 to 0.9\n",
    "# beta_vals = np.round(np.arange(0, 0.9, 0.15), 2)  # Beta values from 0 to 0.9\n",
    "# gamma_vals = np.round(np.arange(0, 0.9, 0.15), 2)  # Gamma values from 0 to 0.9\n",
    "\n",
    "# df_hyp_tuned_exp_smooth_models = tune_exp_smooth_all_park_lots(train_test_splitted_dict=ts_train_test_splitted_dict, cv=4, \n",
    "#                                                                alpha_values=alpha_vals, \n",
    "#                                                                beta_values=beta_vals, \n",
    "#                                                                gamma_values=gamma_vals, \n",
    "#                                                                seasonal_period=126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7044149-3c75-459d-b2ac-0864d545e5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_hyp_tuned_exp_smooth_models.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ca55a3a-a788-46ce-9b8b-1fa3c81001b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_hyp_tuned_exp_smooth_models.cv_rmse.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abba96cc-d9c6-4e1a-b033-ed4d4f97704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving best parameters for the exp_smoothing_models\n",
    "# df_hyp_tuned_exp_smooth_models.to_csv(path_or_buf='./data_artifacts/df_hyp_tuned_params_tes_models.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7b5039-61e6-45ca-b961-966c93fac8a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354f439a-9c41-4c5f-ac92-f65d55154d37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2909c062-5759-4b2d-bd21-c4ccc7107151",
   "metadata": {},
   "source": [
    "# HyperParameter Tuning: SARIMA (using Auto-ARIMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cab3a16d-c101-44b5-96ae-71a325b0aec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_SARIMAX_v2(train_data, \n",
    "                    p_max, q_max, P_max, Q_max, s_val,   \n",
    "                    random_srch=False, random_srch_fits=10):\n",
    "    \n",
    "    # Use pmdarima's auto_arima function to find the best parameters\n",
    "    model = pm.auto_arima(train_data,\n",
    "                          \n",
    "                          \n",
    "                          # Parameter space: Non-seasonal\n",
    "                          start_p=0, max_p=p_max,\n",
    "                          start_q=0, max_q=q_max,\n",
    "  \n",
    "                          # Parameter space: Seasonal\n",
    "                          seasonal=True, \n",
    "                          start_P=0, max_P=P_max,\n",
    "                          start_Q=0, max_Q=Q_max,\n",
    "                          m=s_val,\n",
    " \n",
    "                          # Differencing: Automatically determine non-seasonal and seasonal differencing order\n",
    "                          stationary=False,\n",
    "                          d=0, D=1, \n",
    "                              \n",
    "                          # Whether to perform grid vs random search\n",
    "                          random=random_srch,\n",
    "                          n_fits=random_srch_fits,\n",
    "                              \n",
    "\n",
    "                          trace=True,                 # print the progress   \n",
    "                          error_action='ignore',      # ignore if a model does not work  \n",
    "                          suppress_warnings=True,     # suppress warnings    \n",
    "                          stepwise=True,              # apply stepwise algorithm for faster processing\n",
    "                          maxiter = 25, \n",
    "                          n_jobs=-1                   # Use all processors  \n",
    "                         )\n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f5748e8-74b0-4dec-a2d2-5e06de07d9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_SARIMAX_all_park_lots(train_test_splitted_dict, SARIMAX_tuner_fn):\n",
    "    \n",
    "    tuned_sarima_models_dict = {'ps_idx':[], 'tuned_model':[], 'pdq':[], 'PDQs':[], 'forecast_mae':[], 'forecast_rmse':[]}\n",
    "\n",
    "    for ps_idx in list(train_test_splitted_dict.keys())[:]:\n",
    "\n",
    "        print(f'Finding Best Parameters for Park_Space_ID: {ps_idx}')\n",
    "        print('-'*50)\n",
    "        train_series = ts_train_test_splitted_dict[ps_idx]['train']\n",
    "        test_series = ts_train_test_splitted_dict[ps_idx]['test']\n",
    "        print(f'Train series length: {train_series.shape},  Test series length: {test_series.shape}')\n",
    "\n",
    "        # Start Tuning\n",
    "        print('-'*25+'Model Fitting start'+'-'*25)\n",
    "        start_time = time.time()\n",
    "        tuned_sarimax_model_with_summary = SARIMAX_tuner_fn(train_data=train_series,    \n",
    "                                                            p_max=1, q_max=1, # Non-Seasonal AR, MA orders set to [0, 1]\n",
    "                                                            P_max=0, Q_max=0, # Seasonal AR, MA orders set to 0  \n",
    "                                                            s_val=18*7,       # Seasonal Period  \n",
    "                                                            random_srch=True, random_srch_fits=10)\n",
    "        end_time = time.time()\n",
    "        time_taken = (end_time-start_time)\n",
    "        print('-'*25+'Model Fitting end'+'-'*25)\n",
    "        # End Tuning\n",
    "\n",
    "        \n",
    "        print(f'Best non-seasonal order: (p,d,q): {tuned_sarimax_model_with_summary.order}')\n",
    "        print(f'Best seasonal order: (P,D,Q): {tuned_sarimax_model_with_summary.seasonal_order}')\n",
    "\n",
    "        # Forecasting\n",
    "        tuned_sarimax_model_forecast = tuned_sarimax_model_with_summary.predict(n_periods=len(test_series))\n",
    "        tuned_sarimax_model_forecast.index = test_series.index\n",
    "        fr_mae, fr_rmse = ts_performance_metrics(actual=test_series, predicted=tuned_sarimax_model_forecast)\n",
    "        \n",
    "        # Storing the results\n",
    "        tuned_sarima_models_dict['ps_idx'].append(ps_idx)\n",
    "        tuned_sarima_models_dict['tuned_model'].append(tuned_sarimax_model_with_summary)\n",
    "        tuned_sarima_models_dict['pdq'].append(tuned_sarimax_model_with_summary.order)\n",
    "        tuned_sarima_models_dict['PDQs'].append(tuned_sarimax_model_with_summary.seasonal_order)\n",
    "        tuned_sarima_models_dict['forecast_mae'].append(fr_mae)\n",
    "        tuned_sarima_models_dict['forecast_rmse'].append(fr_rmse)\n",
    "\n",
    "        print(('-'*50)+f'Completed, Time Taken: {round(time_taken, 2)} '+('-'*50))\n",
    "        \n",
    "        \n",
    "    return tuned_sarima_models_dict\n",
    "\n",
    "# hyp_tuned_sarima_models_dict = tune_SARIMAX_all_park_lots(train_test_splitted_dict=ts_train_test_splitted_dict, \n",
    "#                                                           SARIMAX_tuner_fn=tune_SARIMAX_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b5f5af-304a-44df-8d32-3c10581429d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3724f0aa-b06b-4d57-a739-4082de9fd1d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cbf8ed-2d61-44c2-8c32-587ee5457ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75af39a1-8e70-448d-89c7-7651a8140da0",
   "metadata": {},
   "source": [
    "# Helper Functions: Regression Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9eab9c54-8298-45ed-94b2-ff9a35072aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "def features_lagged(df_inp):\n",
    "    \n",
    "    df = df_inp.copy()\n",
    "    \n",
    "\n",
    "    # Lagged Features\n",
    "    df['lag_1'] = df['Occupancy_Rate'].shift(1)\n",
    "    df['lag_2'] = df['Occupancy_Rate'].shift(2)\n",
    "    df['lag_3'] = df['Occupancy_Rate'].shift(3)\n",
    "\n",
    "    df['lag_18'] = df['Occupancy_Rate'].shift(18)\n",
    "    df['lag_19'] = df['Occupancy_Rate'].shift(19)\n",
    "    df['lag_20'] = df['Occupancy_Rate'].shift(20)\n",
    "    \n",
    "    \n",
    "    # Encoding Week-Weekend as numeric\n",
    "    df['isWeekend'] = df['isWeekend'].astype('int')\n",
    "    \n",
    "    \n",
    "    # Impute Missing data with mean imputation\n",
    "    target_mean = df.Occupancy_Rate.mean()\n",
    "\n",
    "    df.lag_1.fillna(target_mean, inplace=True)\n",
    "    df.lag_2.fillna(target_mean, inplace=True)\n",
    "    df.lag_3.fillna(target_mean, inplace=True)\n",
    "\n",
    "    df.lag_18.fillna(target_mean, inplace=True)\n",
    "    df.lag_19.fillna(target_mean, inplace=True)\n",
    "    df.lag_20.fillna(target_mean, inplace=True)\n",
    "    \n",
    "    \n",
    "    # # Setting timestamp as index: Not required (Doing it earlier)\n",
    "    # df.set_index('TimeStamp', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Feature Scaling\n",
    "def feat_scaler(X_train_inp, X_test_inp):\n",
    "    \n",
    "    std_scaler = StandardScaler()\n",
    "    X_train_inp_scl = std_scaler.fit_transform(X_train_inp)\n",
    "    X_test_inp_scl = std_scaler.transform(X_test_inp)\n",
    "    \n",
    "    return X_train_inp_scl, X_test_inp_scl\n",
    "\n",
    "\n",
    "# X-y splitter\n",
    "def preprocess_pipe(df_train, df_test):\n",
    "    \n",
    "    X_train = df_train.drop(['Occupancy_Rate'], axis=1)\n",
    "    y_train = df_train['Occupancy_Rate']\n",
    "\n",
    "    X_test = df_test.drop(['Occupancy_Rate'], axis=1)\n",
    "    y_test = df_test['Occupancy_Rate']\n",
    "    \n",
    "    X_train_scl, X_test_scl = feat_scaler(X_train_inp=X_train, X_test_inp=X_test)\n",
    "    \n",
    "    return X_train, X_test, X_train_scl, X_test_scl, y_train, y_test\n",
    "\n",
    "\n",
    "# Generate Regression Report\n",
    "def get_regression_report(model, df_train, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    sep_length = 50\n",
    "    \n",
    "    print('REGRESSION REPORT')\n",
    "    print('-'*sep_length)\n",
    "    \n",
    "    # Fit regression model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_train_pred = model.predict(X_train)\n",
    "    # y_train_pred = pd.Series(y_train_pred)\n",
    "    # y_train_pred.index = y_train.index\n",
    "    \n",
    "    y_test_pred = model.predict(X_test)\n",
    "    # y_test_pred = pd.Series(y_test_pred)\n",
    "    # y_test_pred.index = y_test.index\n",
    "    \n",
    "    # Model Evaluation metrics\n",
    "    train_rmse = round(mse(y_train, y_train_pred)**0.5, 3)\n",
    "    test_rmse = round(mse(y_test, y_test_pred)**0.5, 3)\n",
    "    \n",
    "    train_mae = round(mae(y_train, y_train_pred), 3)\n",
    "    test_mae = round(mae(y_test, y_test_pred), 3)\n",
    "    \n",
    "    # Store Fitted Model & Metrics\n",
    "    # metrics_arr = np.array([])\n",
    "    metrics_arr = np.array([train_rmse, train_mae, test_rmse, test_mae])\n",
    "    \n",
    "    print('Training RMSE:', train_rmse)\n",
    "    print('Testing RMSE:', test_rmse)\n",
    "    print('-'*sep_length)\n",
    "    \n",
    "    # # Feature importances\n",
    "    # ser_feat_imp = pd.Series(model.feature_importances_, index=df_train.columns).sort_values(ascending=False)\n",
    "    # # Plot feature importances\n",
    "    # plt.figure(figsize=(20, 4))\n",
    "    # plt.bar(ser_feat_imp.index, ser_feat_imp.values)\n",
    "    # plt.xticks(rotation=90)\n",
    "    # plt.show()\n",
    "    \n",
    "    return metrics_arr\n",
    "\n",
    "\n",
    "# Plot output post model training\n",
    "def plot_test_set_org_predict(y_test, y_pred):\n",
    "    \n",
    "    y_pred_ser = pd.Series(y_pred)\n",
    "    y_pred_ser.index = y_test.index\n",
    "    \n",
    "    plt.figure(figsize=(20, 4))\n",
    "    y_test.plot(marker='o', label='TestSet-Original')\n",
    "    y_pred_ser.plot(marker='o', label='TestSet-Predicted')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "def get_cross_val_score_summary(model, X_train, y_train, cv=3, scoring='neg_mean_squared_error'):\n",
    "    \n",
    "    cross_val = cross_val_score(model, X_train, y_train, cv=cv, scoring=scoring)\n",
    "    # print('-'*70)\n",
    "    # print(f'Cross validation Score Summary: #Folds:{cv}, Score:{scoring}')\n",
    "    # print('-'*70)\n",
    "    \n",
    "    mean_cv_rmse = np.mean(((-1)*cross_val)**0.5)\n",
    "    \n",
    "    return mean_cv_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8642f049-f19b-4398-ac4a-48291f767e7a",
   "metadata": {},
   "source": [
    "# HyperParameter Tuning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2633e2c-6f99-47f6-b976-c63f0e785682",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7185a854-e2e0-439d-ac53-378e9f5dc656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp_param_tune_rf(X_train_scl, y_train):\n",
    "\n",
    "    rfr_parameters = {\n",
    "        \"n_estimators\":[50, 100, 150, 200],\n",
    "        \"max_features\":[2, 4, 7, 10, 13],\n",
    "        \"max_depth\":[6, 9, 12, 15],\n",
    "        'criterion' :['squared_error',],\n",
    "        \"ccp_alpha\":[0.0001, 0.001, 0.01],\n",
    "        \"random_state\":[42]\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    rfr = RandomForestRegressor()\n",
    "\n",
    "    # grid_search1 = GridSearchCV(\n",
    "    #     estimator = rfr,\n",
    "    #     param_grid = rfr_parameters,\n",
    "    #     scoring = \"neg_mean_squared_error\",\n",
    "    #     n_jobs = -1,\n",
    "    #     refit=True,               \n",
    "    #     cv=3,\n",
    "    #     verbose=1\n",
    "    #     # return_train_score=False\n",
    "    # )\n",
    "\n",
    "    rand_search1 = RandomizedSearchCV(\n",
    "        estimator = rfr,\n",
    "        param_distributions = rfr_parameters,\n",
    "        n_iter=100,\n",
    "        scoring = \"neg_mean_squared_error\",\n",
    "        n_jobs = -1,\n",
    "        refit=True,               \n",
    "        cv=3,\n",
    "        verbose=1\n",
    "        # return_train_score=False\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # grid_search1.fit(X_train_scl, y_train)\n",
    "    rand_search1.fit(X_train_scl, y_train)\n",
    "\n",
    "    \n",
    "    return rand_search1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a622e32-8973-4ba4-882b-58c2c8e6ce9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Regression Models: Random Forest\n",
    "def find_best_params_rf(reg_final_train_test_splitted_dict):\n",
    "    \n",
    "    fitted_models_dict = {'ps_idx':[], \n",
    "                          'random_state':[],\n",
    "                          'criterion':[],\n",
    "                          'n_estimators':[],\n",
    "                          'max_features':[],\n",
    "                          'max_depth':[],\n",
    "                          'ccp_alpha':[],\n",
    "                          'cv_rmse':[]\n",
    "                         }\n",
    "\n",
    "    \n",
    "    for ps_idx in list(reg_final_train_test_splitted_dict.keys())[:]:\n",
    "        \n",
    "        print(f'PARK SPACE: {ps_idx}')\n",
    "        \n",
    "        df_train_ps = reg_final_train_test_splitted_dict[ps_idx]['train']\n",
    "        df_test_ps = reg_final_train_test_splitted_dict[ps_idx]['test']\n",
    "        \n",
    "        X_train, X_test, X_train_scl, X_test_scl, y_train, y_test = preprocess_pipe(df_train=df_train_ps, \n",
    "                                                                                    df_test=df_test_ps)\n",
    "        \n",
    "        # Best Parameter post hyper-parameter tuning\n",
    "        start_time = time.time()\n",
    "        best_params_ps = hyp_param_tune_rf(X_train_scl=X_train_scl, y_train=y_train)\n",
    "        \n",
    "        \n",
    "        # Initializing RF model using best params\n",
    "        rfr = RandomForestRegressor(random_state=best_params_ps['random_state'], \n",
    "                                    criterion=best_params_ps['criterion'], \n",
    "                                    n_estimators=best_params_ps['n_estimators'], \n",
    "                                    max_features=best_params_ps['max_features'], \n",
    "                                    max_depth=best_params_ps['max_depth'], \n",
    "                                    ccp_alpha=best_params_ps['ccp_alpha'])\n",
    "        \n",
    "        # # Fitting Regression model\n",
    "        # rfr.fit(X_train_scl, y_train)\n",
    "        \n",
    "        # Getting Mean CV RMSE\n",
    "        mean_cv_rmse = get_cross_val_score_summary(model=rfr, X_train=X_train_scl, y_train=y_train, \n",
    "                                                   cv=5, scoring='neg_mean_squared_error')\n",
    "        \n",
    "        \n",
    "        end_time = time.time()\n",
    "        print('-'*25+f'Completed Tuning: {round(end_time-start_time, 3)}'+'-'*25)\n",
    "        \n",
    "        \n",
    "        # Saving the best parameters\n",
    "        fitted_models_dict['ps_idx'].append(ps_idx)\n",
    "        fitted_models_dict['random_state'].append(best_params_ps['random_state'])\n",
    "        fitted_models_dict['criterion'].append(best_params_ps['criterion'])\n",
    "        fitted_models_dict['n_estimators'].append(best_params_ps['n_estimators'])\n",
    "        fitted_models_dict['max_features'].append(best_params_ps['max_features'])\n",
    "        fitted_models_dict['max_depth'].append(best_params_ps['max_depth'])\n",
    "        fitted_models_dict['ccp_alpha'].append(best_params_ps['ccp_alpha'])\n",
    "        fitted_models_dict['cv_rmse'].append(mean_cv_rmse)\n",
    "        \n",
    "        \n",
    "    return pd.DataFrame(fitted_models_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a576b6f5-2a92-4ec2-adf1-352183f14e5d",
   "metadata": {},
   "source": [
    "## XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1424a4d3-a5cf-4ee3-ba31-4962592dc30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp_param_tune_xgb(X_train_scl, y_train):\n",
    "    \n",
    "    \n",
    "    xgbr_parameters = {\n",
    "        \"n_estimators\":[50, 100, 150, 200],\n",
    "        \"learning_rate\": [0.05, 0.1, 0.2, 0.3],\n",
    "        \"gamma\": [0, 0.25, 0.5, 0.75, 1, 1.25],\n",
    "        \"max_depth\":[2, 4, 6, 8],\n",
    "        'subsample': [0.3, 0.5, 0.7, 0.9],\n",
    "        \"colsample_bytree\" : [0.3, 0.5, 0.7, 0.9],\n",
    "        \"random_state\":[42]\n",
    "    }\n",
    "\n",
    "\n",
    "    xgbr = xgb.XGBRegressor()\n",
    "\n",
    "    # grid_search2 = GridSearchCV(\n",
    "    #     estimator = xgbr,\n",
    "    #     param_grid = xgbr_parameters,\n",
    "    #     scoring = \"neg_mean_squared_error\",\n",
    "    #     n_jobs = -1,\n",
    "    #     refit=True,               \n",
    "    #     cv=3,\n",
    "    #     verbose=1\n",
    "    #     # return_train_score=False\n",
    "    # )\n",
    "\n",
    "    rand_search2 = RandomizedSearchCV(\n",
    "        estimator = xgbr,\n",
    "        param_distributions = xgbr_parameters,\n",
    "        n_iter=100,\n",
    "        scoring = \"neg_mean_squared_error\",\n",
    "        n_jobs = -1,\n",
    "        refit=True,               \n",
    "        cv=3,\n",
    "        verbose=1\n",
    "        # return_train_score=False\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # grid_search2.fit(X_train_scl, y_train)\n",
    "    rand_search2.fit(X_train_scl, y_train)\n",
    "\n",
    "    \n",
    "    return rand_search2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5caf9a85-fe30-4826-94a2-b63346f1190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Regression Models: XGBoost\n",
    "def find_best_params_xgb(reg_final_train_test_splitted_dict):\n",
    "    \n",
    "    fitted_models_dict = {'ps_idx':[], \n",
    "                          'random_state':[],\n",
    "                          'n_estimators':[],\n",
    "                          'learning_rate':[],\n",
    "                          'gamma':[],\n",
    "                          'max_depth':[],\n",
    "                          'subsample':[],\n",
    "                          'colsample_bytree':[],\n",
    "                          'cv_rmse':[]\n",
    "                         }\n",
    "\n",
    "    \n",
    "    for ps_idx in list(reg_final_train_test_splitted_dict.keys())[:]:\n",
    "        \n",
    "        print(f'PARK SPACE: {ps_idx}')\n",
    "        \n",
    "        df_train_ps = reg_final_train_test_splitted_dict[ps_idx]['train']\n",
    "        df_test_ps = reg_final_train_test_splitted_dict[ps_idx]['test']\n",
    "        \n",
    "        X_train, X_test, X_train_scl, X_test_scl, y_train, y_test = preprocess_pipe(df_train=df_train_ps, \n",
    "                                                                                    df_test=df_test_ps)\n",
    "        \n",
    "        # Best Parameter post hyper-parameter tuning\n",
    "        start_time = time.time()\n",
    "        best_params_ps = hyp_param_tune_xgb(X_train_scl=X_train_scl, y_train=y_train)\n",
    "        # print(best_params_ps)\n",
    "        \n",
    "        \n",
    "        # Initializing RF model using best params\n",
    "        xgbr = xgb.XGBRegressor(random_state=best_params_ps['random_state'],   \n",
    "                                n_estimators=best_params_ps['n_estimators'], \n",
    "                                learning_rate=best_params_ps['learning_rate'],\n",
    "                                gamma=best_params_ps['gamma'],     \n",
    "                                max_depth=best_params_ps['max_depth'],\n",
    "                                subsample=best_params_ps['subsample'],\n",
    "                                colsample_bytree=best_params_ps['colsample_bytree'])\n",
    "        \n",
    "        \n",
    "        # # Fitting Regression model\n",
    "        # xgbr.fit(X_train_scl, y_train)\n",
    "        \n",
    "        # Getting Mean CV RMSE\n",
    "        mean_cv_rmse = get_cross_val_score_summary(model=xgbr, X_train=X_train_scl, y_train=y_train, \n",
    "                                                   cv=5, scoring='neg_mean_squared_error')\n",
    "        \n",
    "        \n",
    "        end_time = time.time()\n",
    "        print('-'*25+f'Completed Tuning: {round(end_time-start_time, 3)}'+'-'*25)\n",
    "        \n",
    "        \n",
    "        # Saving the best parameters\n",
    "        fitted_models_dict['ps_idx'].append(ps_idx)\n",
    "        fitted_models_dict['random_state'].append(best_params_ps['random_state'])\n",
    "        fitted_models_dict['n_estimators'].append(best_params_ps['n_estimators'])\n",
    "        fitted_models_dict['learning_rate'].append(best_params_ps['learning_rate'])\n",
    "        fitted_models_dict['gamma'].append(best_params_ps['gamma'])\n",
    "        fitted_models_dict['max_depth'].append(best_params_ps['max_depth'])\n",
    "        fitted_models_dict['subsample'].append(best_params_ps['subsample'])\n",
    "        fitted_models_dict['colsample_bytree'].append(best_params_ps['colsample_bytree'])\n",
    "        fitted_models_dict['cv_rmse'].append(mean_cv_rmse)\n",
    "        \n",
    "        \n",
    "    return pd.DataFrame(fitted_models_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ae98d9-3f5c-45dc-a105-85216b60252c",
   "metadata": {},
   "source": [
    "# HyperParameter Tuning: Random Forest Regressor (Without TES O/P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d70d88f-28c8-43a1-b4b0-4cf0192534d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARK SPACE: 1\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 9.287-------------------------\n",
      "PARK SPACE: 2\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 9.88-------------------------\n",
      "PARK SPACE: 3\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 9.955-------------------------\n",
      "PARK SPACE: 4\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 8.861-------------------------\n",
      "PARK SPACE: 5\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 9.539-------------------------\n",
      "PARK SPACE: 6\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 8.571-------------------------\n",
      "PARK SPACE: 7\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 10.531-------------------------\n",
      "PARK SPACE: 8\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 10.705-------------------------\n",
      "PARK SPACE: 9\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 7.511-------------------------\n",
      "PARK SPACE: 10\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 9.578-------------------------\n",
      "PARK SPACE: 11\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 8.169-------------------------\n",
      "PARK SPACE: 12\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 7.772-------------------------\n",
      "PARK SPACE: 13\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 12.353-------------------------\n",
      "PARK SPACE: 14\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 7.71-------------------------\n",
      "PARK SPACE: 15\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 8.972-------------------------\n",
      "PARK SPACE: 16\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 11.951-------------------------\n",
      "PARK SPACE: 17\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 12.342-------------------------\n",
      "PARK SPACE: 18\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 10.642-------------------------\n",
      "PARK SPACE: 19\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 9.471-------------------------\n",
      "PARK SPACE: 20\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 10.724-------------------------\n",
      "PARK SPACE: 21\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 9.968-------------------------\n",
      "PARK SPACE: 22\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 12.344-------------------------\n",
      "PARK SPACE: 23\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 9.559-------------------------\n",
      "PARK SPACE: 24\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 9.194-------------------------\n",
      "PARK SPACE: 25\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 10.907-------------------------\n",
      "PARK SPACE: 26\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 10.712-------------------------\n",
      "PARK SPACE: 27\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 8.599-------------------------\n"
     ]
    }
   ],
   "source": [
    "df_hyp_tuned_rf_models_v1 = find_best_params_rf(reg_final_train_test_splitted_dict=reg_final_v1_mod_train_test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f5afe7a-add0-4605-9b32-55fb58bcd61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ps_idx</th>\n",
       "      <th>random_state</th>\n",
       "      <th>criterion</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_features</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>ccp_alpha</th>\n",
       "      <th>cv_rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2.376033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>200</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5.035085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>42</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>3.339578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>42</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>150</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4.219668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>42</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>200</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>4.180338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ps_idx  random_state      criterion  n_estimators  max_features  max_depth  \\\n",
       "0       1            42  squared_error           100            10          9   \n",
       "1       2            42  squared_error           200             7         15   \n",
       "2       3            42  squared_error           150            10         15   \n",
       "3       4            42  squared_error           150            13          9   \n",
       "4       5            42  squared_error           200             7         15   \n",
       "\n",
       "   ccp_alpha   cv_rmse  \n",
       "0     0.0001  2.376033  \n",
       "1     0.0001  5.035085  \n",
       "2     0.0010  3.339578  \n",
       "3     0.0001  4.219668  \n",
       "4     0.0010  4.180338  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hyp_tuned_rf_models_v1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "687af6e4-a674-46cd-86d9-eec0edec9220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    27.000000\n",
       "mean      2.619489\n",
       "std       0.928895\n",
       "min       1.001775\n",
       "25%       2.080783\n",
       "50%       2.387356\n",
       "75%       2.921339\n",
       "max       5.035085\n",
       "Name: cv_rmse, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hyp_tuned_rf_models_v1.cv_rmse.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c60d2261-28ea-4db6-a3f7-ab3914e7b8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving best parameters for the exp_smoothing_models\n",
    "df_hyp_tuned_rf_models_v1.to_csv(path_or_buf='./data_artifacts/df_hyp_tuned_params_rf_models_v1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274dc02b-fa4a-4b2a-91fb-46ab66859849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eff0ca0f-4205-4781-b69b-8cbef0876ee4",
   "metadata": {},
   "source": [
    "# HyperParameter Tuning: Random Forest Regressor (With TES O/P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12db6762-5ab2-44b1-af17-bd8c56670ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARK SPACE: 1\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 11.146-------------------------\n",
      "PARK SPACE: 2\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 8.61-------------------------\n",
      "PARK SPACE: 3\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 8.931-------------------------\n",
      "PARK SPACE: 4\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 9.722-------------------------\n",
      "PARK SPACE: 5\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 8.866-------------------------\n",
      "PARK SPACE: 6\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 10.656-------------------------\n",
      "PARK SPACE: 7\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 8.345-------------------------\n",
      "PARK SPACE: 8\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 8.553-------------------------\n",
      "PARK SPACE: 9\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 9.352-------------------------\n",
      "PARK SPACE: 10\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 10.782-------------------------\n",
      "PARK SPACE: 11\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 8.999-------------------------\n",
      "PARK SPACE: 12\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 8.619-------------------------\n",
      "PARK SPACE: 13\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 9.505-------------------------\n",
      "PARK SPACE: 14\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 9.496-------------------------\n",
      "PARK SPACE: 15\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 9.704-------------------------\n",
      "PARK SPACE: 16\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 10.089-------------------------\n",
      "PARK SPACE: 17\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 11.815-------------------------\n",
      "PARK SPACE: 18\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 10.138-------------------------\n",
      "PARK SPACE: 19\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 10.123-------------------------\n",
      "PARK SPACE: 20\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 10.779-------------------------\n",
      "PARK SPACE: 21\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 9.021-------------------------\n",
      "PARK SPACE: 22\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 11.068-------------------------\n",
      "PARK SPACE: 23\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 12.561-------------------------\n",
      "PARK SPACE: 24\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 9.887-------------------------\n",
      "PARK SPACE: 25\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 12.766-------------------------\n",
      "PARK SPACE: 26\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 10.934-------------------------\n",
      "PARK SPACE: 27\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 10.298-------------------------\n"
     ]
    }
   ],
   "source": [
    "df_hyp_tuned_rf_models_v2 = find_best_params_rf(reg_final_train_test_splitted_dict=reg_final_v2_mod_train_test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7006841d-9ed1-49e9-b69e-1e0384b1bc24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ps_idx</th>\n",
       "      <th>random_state</th>\n",
       "      <th>criterion</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_features</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>ccp_alpha</th>\n",
       "      <th>cv_rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>2.078415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>4.785509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>42</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>3.731414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>42</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5.335787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>42</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>200</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4.487857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ps_idx  random_state      criterion  n_estimators  max_features  max_depth  \\\n",
       "0       1            42  squared_error           150            10         15   \n",
       "1       2            42  squared_error           100             4         12   \n",
       "2       3            42  squared_error            50             7         15   \n",
       "3       4            42  squared_error           200            10         12   \n",
       "4       5            42  squared_error           200             4         12   \n",
       "\n",
       "   ccp_alpha   cv_rmse  \n",
       "0     0.0010  2.078415  \n",
       "1     0.0100  4.785509  \n",
       "2     0.0010  3.731414  \n",
       "3     0.0001  5.335787  \n",
       "4     0.0001  4.487857  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hyp_tuned_rf_models_v2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "87776898-b3c2-4b9b-b4ae-5aa3289ed73a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    27.000000\n",
       "mean      2.827754\n",
       "std       1.077564\n",
       "min       1.290961\n",
       "25%       2.131648\n",
       "50%       2.670427\n",
       "75%       3.382621\n",
       "max       5.335787\n",
       "Name: cv_rmse, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hyp_tuned_rf_models_v2.cv_rmse.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "02a70255-e8d6-4587-8280-c9daf51770b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving best parameters for the exp_smoothing_models\n",
    "df_hyp_tuned_rf_models_v2.to_csv(path_or_buf='./data_artifacts/df_hyp_tuned_params_rf_models_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cf10de-320c-4e8d-b2e8-04330743439a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22800539-9f5d-4a71-b085-ae28d09a4171",
   "metadata": {},
   "source": [
    "# HyperParameter Tuning: XGBoost Regressor (Without TES O/P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c121010-dacb-42c0-954f-05586db18949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARK SPACE: 1\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 1.988-------------------------\n",
      "PARK SPACE: 2\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 1.566-------------------------\n",
      "PARK SPACE: 3\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 1.502-------------------------\n",
      "PARK SPACE: 4\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 1.741-------------------------\n",
      "PARK SPACE: 5\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 2.638-------------------------\n",
      "PARK SPACE: 6\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 2.237-------------------------\n",
      "PARK SPACE: 7\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 2.272-------------------------\n",
      "PARK SPACE: 8\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 2.521-------------------------\n",
      "PARK SPACE: 9\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 2.259-------------------------\n",
      "PARK SPACE: 10\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 2.861-------------------------\n",
      "PARK SPACE: 11\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 2.321-------------------------\n",
      "PARK SPACE: 12\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 2.253-------------------------\n",
      "PARK SPACE: 13\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 2.081-------------------------\n",
      "PARK SPACE: 14\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 1.515-------------------------\n",
      "PARK SPACE: 15\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 1.927-------------------------\n",
      "PARK SPACE: 16\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 2.76-------------------------\n",
      "PARK SPACE: 17\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 3.053-------------------------\n",
      "PARK SPACE: 18\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 1.944-------------------------\n",
      "PARK SPACE: 19\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 1.795-------------------------\n",
      "PARK SPACE: 20\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 1.978-------------------------\n",
      "PARK SPACE: 21\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 1.672-------------------------\n",
      "PARK SPACE: 22\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 2.622-------------------------\n",
      "PARK SPACE: 23\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 2.533-------------------------\n",
      "PARK SPACE: 24\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 2.17-------------------------\n",
      "PARK SPACE: 25\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 2.019-------------------------\n",
      "PARK SPACE: 26\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 1.633-------------------------\n",
      "PARK SPACE: 27\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 1.966-------------------------\n"
     ]
    }
   ],
   "source": [
    "df_hyp_tuned_xgb_models_v3 = find_best_params_xgb(reg_final_train_test_splitted_dict=reg_final_v1_mod_train_test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7358e90-53a2-4536-89e1-1ef5313eba9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ps_idx</th>\n",
       "      <th>random_state</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>cv_rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>200</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3.056324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>200</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.852581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>42</td>\n",
       "      <td>100</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>4.512991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>42</td>\n",
       "      <td>100</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>5.094888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>42</td>\n",
       "      <td>200</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.099668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ps_idx  random_state  n_estimators  learning_rate  gamma  max_depth  \\\n",
       "0       1            42           200           0.30   1.00          2   \n",
       "1       2            42           200           0.05   1.25          2   \n",
       "2       3            42           100           0.20   0.50          2   \n",
       "3       4            42           100           0.10   0.00          4   \n",
       "4       5            42           200           0.05   1.00          6   \n",
       "\n",
       "   subsample  colsample_bytree   cv_rmse  \n",
       "0        0.9               0.3  3.056324  \n",
       "1        0.7               0.7  4.852581  \n",
       "2        0.7               0.9  4.512991  \n",
       "3        0.3               0.7  5.094888  \n",
       "4        0.5               0.7  4.099668  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hyp_tuned_xgb_models_v3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "63a38655-fb55-4ce9-8b93-86eda999a24c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    27.000000\n",
       "mean      2.769192\n",
       "std       1.009056\n",
       "min       1.032222\n",
       "25%       2.103388\n",
       "50%       2.558400\n",
       "75%       3.084212\n",
       "max       5.094888\n",
       "Name: cv_rmse, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hyp_tuned_xgb_models_v3.cv_rmse.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cffe896f-1c7e-4067-b893-c31de21df040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving best parameters for the exp_smoothing_models\n",
    "df_hyp_tuned_xgb_models_v3.to_csv(path_or_buf='./data_artifacts/df_hyp_tuned_params_xgb_models_v3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd542fb4-dc50-4b1b-ae7b-913e047952e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c49d4d74-9e7b-4c5c-be85-82a75c97b7c7",
   "metadata": {},
   "source": [
    "# HyperParameter Tuning: XGBoost Regressor (With TES O/P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "208dc152-d312-446a-97e0-1c6368ba5f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARK SPACE: 1\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 1.775-------------------------\n",
      "PARK SPACE: 2\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 2.25-------------------------\n",
      "PARK SPACE: 3\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 3.028-------------------------\n",
      "PARK SPACE: 4\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 1.987-------------------------\n",
      "PARK SPACE: 5\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 2.617-------------------------\n",
      "PARK SPACE: 6\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 2.839-------------------------\n",
      "PARK SPACE: 7\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 2.001-------------------------\n",
      "PARK SPACE: 8\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 2.915-------------------------\n",
      "PARK SPACE: 9\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 1.713-------------------------\n",
      "PARK SPACE: 10\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 3.017-------------------------\n",
      "PARK SPACE: 11\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 2.955-------------------------\n",
      "PARK SPACE: 12\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 1.544-------------------------\n",
      "PARK SPACE: 13\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 2.042-------------------------\n",
      "PARK SPACE: 14\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 1.696-------------------------\n",
      "PARK SPACE: 15\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 2.91-------------------------\n",
      "PARK SPACE: 16\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 3.308-------------------------\n",
      "PARK SPACE: 17\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 2.845-------------------------\n",
      "PARK SPACE: 18\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 1.882-------------------------\n",
      "PARK SPACE: 19\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 2.191-------------------------\n",
      "PARK SPACE: 20\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 2.67-------------------------\n",
      "PARK SPACE: 21\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 1.81-------------------------\n",
      "PARK SPACE: 22\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 2.651-------------------------\n",
      "PARK SPACE: 23\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 1.985-------------------------\n",
      "PARK SPACE: 24\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 3.214-------------------------\n",
      "PARK SPACE: 25\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 2.703-------------------------\n",
      "PARK SPACE: 26\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 2.32-------------------------\n",
      "PARK SPACE: 27\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "-------------------------Completed Tuning: 1.991-------------------------\n"
     ]
    }
   ],
   "source": [
    "df_hyp_tuned_xgb_models_v4 = find_best_params_xgb(reg_final_train_test_splitted_dict=reg_final_v2_mod_train_test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec61db6c-4e7f-4b7d-bc3b-c99132e40e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ps_idx</th>\n",
       "      <th>random_state</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>cv_rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>200</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.7</td>\n",
       "      <td>2.158876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>150</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>4.965143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>42</td>\n",
       "      <td>200</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>3.707578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>42</td>\n",
       "      <td>100</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>5.731208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>42</td>\n",
       "      <td>150</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.462775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ps_idx  random_state  n_estimators  learning_rate  gamma  max_depth  \\\n",
       "0       1            42           200           0.10    1.0          2   \n",
       "1       2            42           150           0.05    0.0          6   \n",
       "2       3            42           200           0.05    1.0          6   \n",
       "3       4            42           100           0.10    0.5          4   \n",
       "4       5            42           150           0.10    0.5          6   \n",
       "\n",
       "   subsample  colsample_bytree   cv_rmse  \n",
       "0        0.9               0.7  2.158876  \n",
       "1        0.3               0.9  4.965143  \n",
       "2        0.5               0.7  3.707578  \n",
       "3        0.7               0.7  5.731208  \n",
       "4        0.5               0.7  4.462775  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hyp_tuned_xgb_models_v4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4e429de3-89c1-4626-81d1-bf7a04672d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    27.000000\n",
       "mean      2.919733\n",
       "std       1.073855\n",
       "min       1.380360\n",
       "25%       2.274084\n",
       "50%       2.642384\n",
       "75%       3.431730\n",
       "max       5.731208\n",
       "Name: cv_rmse, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hyp_tuned_xgb_models_v4.cv_rmse.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fb9d816f-8182-412a-bea1-bf953a12760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving best parameters for the exp_smoothing_models\n",
    "df_hyp_tuned_xgb_models_v4.to_csv(path_or_buf='./data_artifacts/df_hyp_tuned_params_xgb_models_v4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775ee5a2-1e48-4b10-9e01-3cb78138b755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d92e577-9329-4dcc-a232-f7999bef9180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afeaccf-419b-4d5c-9ace-dfb502fe401a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697020bf-0713-4015-bf76-37808a191c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
